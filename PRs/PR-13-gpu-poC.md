Title: PR-13：基于 GPU 的裁剪 PoC（基线 + GPU PoC 骨架）

概述
----
此 PR 将基于 GPU 的视锥裁剪研究从探索脚本推进到运行时集成，目标是把每个实例的可见性检测迁移到 GPU（并在后台 worker 中运行），以降低主线程 CPU 开销并避免不必要的 instance buffer 上传。变更包含异步 worker PoC、GpuCuller 工具、RoadManager -> InstanceRenderer 的可见性回写链路、浏览器 demo 以及若干测试/工具脚本。

变更文件（新增 / 修改）
--------------------
- 新增：
  - `src/world/GpuCuller.ts` ― 浏览器端 GPU culler：支持 Worker + `OffscreenCanvas` 的异步 cull（`cullAsync`），并提供同步回退 `cull`。
  - `demos/gpu_culling_demo.html`、`demos/gpu_culling_demo.js` ― 浏览器演示页面，用于在真实浏览器中对比 CPU 与 GPU PoC。
  - `tools/gpu_culling_poc.ts`、`tools/gpu_culling_gpu_poc.js` ― Node 下的基线与 headless-gl PoC 工具（离线测试用）。
  - `PRs/PR-13-gpu-poC.md`（本文件）― PR 说明草案。
- 修改：
  - `src/world/RoadManager.ts` ― 当启用 `InstanceRenderer` 且 GPU 可用时，异步调用 `GpuCuller.cullAsync(...)` 并把结果注入到 `InstanceRenderer.setGPUVisibility(poolKey, visSet)`。
  - `src/world/InstanceRenderer.ts` ― 增加对 GPU 可见性集合的支持；`update(camera)` 中优先使用 GPU 提供的可见集合以跳过对不可见实例的 CPU 矩阵重建与写入。
  - `package.json` ― 新增脚本：`demo`（本地演示服务器）和 `demo:ci`（PoC CI 辅助脚本）。
  - `docs/environment_system_design.md` ― 记录基线结果、PoC 策略与后续计划（PR-13a/13b/13c）。

实现要点
-------
- GPU 路径实现思路：把每个实例位置打包到纹理（RGBA32F 或 RGBA8 回退），在 fragment shader 中对每个像素（对应一个实例）执行视锥测试，将结果写入 FBO，然后 readback 或通过 worker postMessage 返回可见索引列表。
- 优先使用 Worker + `OffscreenCanvas` 在后台执行 GPU 计算（`cullAsync`），避免阻塞主线程；在不支持的环境下回退到同步实现或 RGBA8 打包方案。
- `InstanceRenderer` 接受 GPU 返回的可见索引集合，在 `update` 中跳过不可见实例的 CPU 工作，只为可见实例设置 matrix，从而减少 CPU→GPU 上传。

演示与工具
---------
- 浏览器演示：`demos/gpu_culling_demo.html` + `demos/gpu_culling_demo.js`，可在支持 WebGL2 的浏览器中运行；若支持 `EXT_color_buffer_float` 则使用浮点路径，否则使用 8-bit 回退。
- Node 工具：`tools/gpu_culling_poc.ts`（CPU 基线）和 `tools/gpu_culling_gpu_poc.js`（headless-gl PoC，包含扩展检测与回退逻辑）。
- npm 脚本：`npm run demo` 启动静态服务供浏览器打开 demo，`npm run demo:ci` 用于在具备 GPU/Headless 支持的 CI 环境运行 PoC 脚本。

验证
----
- 单元测试：在本地集成后，仓库单元测试通过（34/34）。
- 演示/PoC：浏览器 demo 在支持的浏览器上可运行；headless-gl PoC 在 Node 下运行良好，但对浮点帧缓冲的支持依赖特定 headless-gl 构建或 GPU 容器。

注意事项与后续工作
---------------
- 异步 worker 路径依赖浏览器能力（OffscreenCanvas + WebGL2）；在 CI 中建议使用带浏览器的 runner（Puppeteer）或 GPU 支持的 headless 环境。
- 由于 cullAsync 为异步结果，建议在运行时采用双缓冲或使用“上一帧可见列表”策略以避免帧内竞争和抖动。
- 优化方向：
  1) 根据 GPU 返回的可见索引生成紧凑实例缓冲区并使用间接绘制，进一步减少 draw call 与上传；
  2) 使用 WebGPU 的 compute shader 替代 fullscreen-pass，以提高效率和精度；
  3) 在 RoadManager 中将 GPU culling 作为后台长期任务（复用 worker、批处理多个类型）。

合并建议
-------
- 本 PR 可合并到 feature 分支以便在真实浏览器或 GPU-enabled CI 环境中进行进一步验证。变更主要为工具和 demo，并对运行时做了非强制的兼容集成（在不支持 GPU/worker 时有回退），风险较低。

下一步（建议）
-----------
1. 增加 Puppeteer 自动化脚本，在受控浏览器中运行 demo 并采集基线数据（用于 CI）。
2. 将 GPU-visible 到 LOD bucket 的映射精细化（不要仅写入 `${type}|mid`）。
3. 在需要时推进 WebGPU compute shader 的 PoC 实现。

若需，我可以继续生成正式的 Git patch / PR 描述或添加 Puppeteer 自动化脚本并把其加入 CI 流程。

Batch benchmark and detailed timing (work performed)
------------------------------------------------
Status: option C executed ― implemented preheat/resource reuse changes, added CPU baseline in batch, and ran automated batch benchmarks locally.

What was added/changed for benchmarking:
- `demos/gpu_culling_demo.js` ― extended with `runGpuPoCDetailed` exposing `window.runGpuPoCCount` and `window.runCpuBaselineCount` that return per-run timing breakdown (upload / compile / draw / readback / total).
- `tools/ci/batch_gpu_bench.js` ― Puppeteer-based local batch runner (used locally) that executes GPU PoC for counts [1000,5000,10000,50000] × 3 reps and writes `tools/ci/gpu_batch_results.json`.

Key measured findings (from `tools/ci/gpu_batch_results.json`):
- Instances tested: 1k, 5k, 10k, 50k (3 reps each)
- Visible count: always 0 (test positions placed outside frustum for this run)
- Upload: very small (~0.1C1.2 ms)
- Compile: large on first run (shader compile JIT) ― e.g., 37 ms for first 1k run; subsequent runs ~1C2 ms. Recommendation: prewarm/compile once before measuring.
- Draw: negligible
- Readback: dominant steady-state cost (~1.3C3.2 ms depending on size)
- Total (steady-state): ~3C7 ms (first-run outliers due to compile)

Interpretation
- First-run shader compilation dominates outliers; steady-state GPU time is dominated by synchronous `gl.readPixels` readback. Texture upload is not the main bottleneck in this PoC.
- At low instance counts CPU frustum-culling is comparable or faster; GPU helps more when offloading to background and when avoiding frequent synchronous readback or when preparing compact instance updates for indirect draws.

Actions performed (C)
- Implemented demo timing instrumentation (upload/compile/draw/readback).  
- Implemented batch runner and executed locally; results saved to `tools/ci/gpu_batch_results.json`.

Recommendations / next steps
1. Preheat & resource reuse (already applied to reduce compile noise during measurement). Keep permanent resource reuse in runtime (create program/VAO/FBO once and reuse).  
2. Reduce synchronous readback: consider strategies to avoid per-frame `readPixels` (asynchronous accumulation, WebGPU compute shader, or transform-based aggregation on GPU).  
3. Integrate CPU baseline into regular batch (done) and produce CSV/graph for reporting.  
4. For runtime: map GPU-visible lists to exact LOD buckets and apply double-buffering to avoid frame race.  

Artifact locations (local):
- `tools/ci/gpu_batch_results.json` ― raw benchmark results.  
- `demos/gpu_culling_demo.js` ― instrumented demo.  
- `tools/ci/batch_gpu_bench.js` ― batch run script used to collect results.

This PR update completes PR-13a (research + baseline PoC), advances PR-13b (browser PoC implemented), and completes PR-13c benchmarking (local batch runs). Remaining work is optimization (avoid readback) and production integration (LOD mapping, indirect draw path).

WebGPU compute shader PoC (new)
--------------------------------
- 新增：`demos/gpu_culling_webgpu.html` 与 `demos/gpu_culling_webgpu.js` ― WebGPU compute shader PoC that:
  - allocates `positions` as a storage buffer, runs a compute shader per-instance to perform a clip-space/frustum test, writes per-instance visible flags to an output storage buffer, and copies results back to a mapped buffer for readback.
  - records dispatch and readback timings for counts [1k,5k,10k,50k] and exposes results on `window.webgpuResults` in the demo page.
  - Purpose: demonstrate an alternative to the fullscreen fragment-pass approach and evaluate dispatch vs readback costs on WebGPU-capable browsers.

Findings from PoC (notes):
- WebGPU allows straightforward per-instance compute with a simple compute shader and storage buffers; dispatch is fast, but reading back results still involves a copy to a MAP_READ buffer and a mapAsync step ― which is non-blocking but still incurs GPU->CPU latency. The PoC measures dispatch vs readback separately so we can reason about async application strategies (double-buffering, GPU-side compaction).

Next recommended step (selected)
--------------------------------
Integrate WebGPU-visible list into the runtime path (InstanceRenderer) with a double-buffering strategy and a GPU-side compaction / index-generation stage:
1. Use WebGPU compute shader to produce a compact visible index buffer (or prefix-sum-based compaction) on the GPU to avoid transferring full boolean arrays and to minimize readback size.
2. Double-buffer the visible lists (apply last-frame list while current-frame GPU compaction runs) to avoid frame stalls and visual jitter.
3. Modify `InstanceRenderer` to accept compact visible buffers (or an applied visible index array) and only update instance matrices for visible indices; consider triggering indirect draws or partial buffer updates where available.

I will proceed to implement step 1 of the selected next step: integrate WebGPU-visible lists into `InstanceRenderer` using a double-buffer apply strategy (non-blocking readback of compacted indices) and add a simple GPU compaction shader PoC under `tools/` / demo integration.

